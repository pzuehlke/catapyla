{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization and broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 1 $ Vectorization\n",
    "\n",
    "\n",
    "Suppose that we have three-dimensional arrays $ \\mathbf u $ and $ \\mathbf v $\n",
    "and would like to take their inner (dot) product. Here's the most obvious way of doing\n",
    "this in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dimension = 3\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([-1, 0, 1])\n",
    "\n",
    "product = 0\n",
    "for i in range(dimension):\n",
    "    product += u[i] * v[i]\n",
    "print(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach's limitation is that it performs the required arithmetical\n",
    "operations *in sequence*. That is, we first compute the product $ 1 \\cdot (-1) $\n",
    "and add it to our cumulative total; then we add $ 2 \\cdot 0 $; and finally we add\n",
    "$ 3 \\cdot 1 $ to arrive at the result.\n",
    "\n",
    "We can speed up computations of this kind significantly by processing the\n",
    "entire arrays, or at least large chunks of the arrays, at once. This technique\n",
    "is called **vectorization**. It leverages the optimized implementations of vector and\n",
    "matrix operations provided by libraries such as NumPy, which make use of\n",
    "multi-core CPUs or even GPUs (Graphics Processing Units) to execute tasks *in\n",
    "parallel* whenever possible. \n",
    "\n",
    "The vectorized version of the previous example would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([-1, 0, 1])\n",
    "\n",
    "product = np.dot(u, v)  # Compute the dot product of u and v.\n",
    "print(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is certainly more concise and legible. However, because the vectors\n",
    "have only $ 3 $ dimensions, the performance boost is not noticeable. To better\n",
    "appreciate the power of vectorization, let's consider the task of computing\n",
    "the dot product of two vectors having $ 10 $ million dimensions. Here is the\n",
    "vectorized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inner product is 2499843.426373514.\n",
      "Runtime for vectorized version: 4.890680313110352 ms.\n"
     ]
    }
   ],
   "source": [
    "import time  # Module that will allow us to time the computations\n",
    "\n",
    "# Generate two large vectors with random coordinates in [0, 1):\n",
    "dimension = 10**7\n",
    "u = np.random.rand(dimension)  \n",
    "v = np.random.rand(dimension)\n",
    "\n",
    "tic = time.time()\n",
    "prod = np.dot(u, v)\n",
    "toc = time.time()\n",
    "vect_runtime = toc - tic\n",
    "\n",
    "print(f\"The inner product is {prod}.\")\n",
    "print(f\"Runtime for vectorized version: {1000 * vect_runtime} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù The function `time.time()` used above returns a floating point number that\n",
    "represents the time in seconds since January 1st, 1970, 00:00:00 (UTC). The\n",
    "duration of the computation was measured by taking the time before (`tic`) and\n",
    "after (`toc`) it, and then taking the difference. \n",
    "\n",
    "Considering the dimension of the vectors, the computation was pretty fast. Let's\n",
    "now contrast the performance to that of its non-vectorized counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inner product is 2499843.4263739116.\n",
      "Runtime for vectorized version: 1.4287347793579102 s.\n",
      "Or, in miliseconds: 1428.7347793579102\n"
     ]
    }
   ],
   "source": [
    "import time  # A module that will allow us to time the computations\n",
    "\n",
    "tic = time.time()\n",
    "prod = 0\n",
    "for i in range(dimension):\n",
    "    prod += u[i] * v[i]\n",
    "toc = time.time()\n",
    "non_vect_runtime = toc - tic\n",
    "print(f\"The inner product is {prod}.\")\n",
    "print(f\"Runtime for vectorized version: {non_vect_runtime} s.\")\n",
    "print(f\"Or, in miliseconds: {1000 * (non_vect_runtime)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù The slight discrepancy in the value of the dot product between the two\n",
    "versions arises because the non-vectorized method accumulates floating-point\n",
    "arithmetic errors more prominently than the vectorized operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Referring to the example above, compute the precise \n",
    "speedup factor relating the two implementations. Why does this value\n",
    "change everytime you compute it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This discussion shows that vectorization provides a simple way to improve the\n",
    "performance of our code by several orders of magnitude. This is especially\n",
    "crucial in machine learning and computer graphics. These fields frequently\n",
    "involve dealing with vast datasets and complex numerical computations. In such\n",
    "cases, code that does not make use of vectorization is simply not viable.\n",
    "\n",
    "To summarize: *When performing operations on arrays, avoid loops whenever\n",
    "possible; instead, use the built-in vectorized implementations provided by\n",
    "NumPy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 3 $ Vector- and matrix-valued functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate another aspect of vectorization, let $ \\mathbf u = (1, 2, 3) $ and\n",
    "suppose that we need to apply the exponential function to each of the\n",
    "coordinates of $ \\mathbf u $. That is, suppose that we wish to compute $ \\exp(u)\n",
    "= (e^1, e^2, e^3) $. We could simply use a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "u = np.array([1, 2, 3])\n",
    "v = np.zeros(3)  # will hold the result\n",
    "for i in range(3):\n",
    "    v[i] = np.exp(u[i])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is an alternative way that is both simpler and more efficient,\n",
    "namely, to use vectorization, in this case by leveraging to NumPy to apply the\n",
    "exponential to the vector $ u $ as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "v = np.exp(u)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach also works with basic operations that are built into Python, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 9]\n"
     ]
    }
   ],
   "source": [
    "squared_u = u**2\n",
    "print(squared_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.5        0.33333333]\n"
     ]
    }
   ],
   "source": [
    "reciprocal_u = 1 / u\n",
    "print(reciprocal_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Similarly to what we did for the inner product, compare the difference in performance\n",
    "between the vectorized and non-vectorized approaches to computing $ e^u $, where $ u $ is a random\n",
    "vector having a large number of dimensions. What is the speedup factor, approximately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Let $ u = (1, 2, 3) $. Compute:\n",
    "\n",
    "(a) $ \\log(u) $ (that is, the vector whose coordinates are the logarithms of the coordinates of $ u $). *Hint*: Use the `np.log` function.\n",
    "\n",
    "(b) $ \\vert{u}\\vert $ (that is, the vector whose coordinates are the absolute values of the coordinates of $ u $). *Hint:* Use the `np.abs` function.\n",
    "\n",
    "(c) $ \\max\\{u, 3\\} $. *Hint:* Use the `np.max` function.\n",
    "\n",
    "(d) $ \\sin\\big(\\tfrac{\\pi u}{2}\\big) $. *Hint:* Use the `np.sin` function.\n",
    "\n",
    "How did you interpret the vectors of items (c) and (d)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 8 $ Basic terminology of probability theory\n",
    "\n",
    "NumPy provides a comprehensive toolkit for random number generation.  However,\n",
    "before discussing these tools, let us first briefly explain some basic\n",
    "terminology from probability theory that are necessary to grasp what is going on\n",
    "behind the scenes.\n",
    "\n",
    "### $ 8.1 $ A simple example\n",
    "\n",
    "Consider an experiment which consists of rolling a six-faced die. Then the set\n",
    "of all possible _outcomes_ is given by $ \\Omega = \\{1, 2, \\cdots, 6\\} $.  This\n",
    "is our _sample space_ in this case. An _event_, such as rolling an even number,\n",
    "is simply a subset of $ \\Omega $, in this case $ E = \\{2, 4, 6\\} $. Another\n",
    "event would be rolling a face greater than $ 4 $, described by $ A = \\{5, 6\\} $.\n",
    "\n",
    "If the die is perfectly balanced, so that each outcome is equally likely, then\n",
    "the _probability distribution_ on our sample space is the function $ f \\colon\n",
    "\\Omega \\to [0, 1] $ that assigns the value $ \\frac{1}{6} $ to each face.\n",
    "Actually, this is a very special distribution called the _uniform distribution_.\n",
    "However, in practice the die will never be perfectly balanced, so that some\n",
    "outcomes (faces) may be more probable than others. It is the purpose of the\n",
    "probability distribution to describe how probabilities are allocated among the\n",
    "possible outcomes.\n",
    "\n",
    "The preceding example illustrates a _finite_ sample space. However, many\n",
    "interesting phenomena, such as measuring the luminosity of a star or the\n",
    "height of an individual, are more adequately modeled by continuous sample spaces.\n",
    "In such cases, the probability of any single outcome is zero. The probability\n",
    "of an event $ A $ is obtained by _integrating_ the probability distribution over\n",
    "$ A $.  The formal definitions below will cover both cases.\n",
    "\n",
    "### ‚ö° $ 8.2 $ Formal definitions\n",
    "\n",
    "Let $ \\Omega $ be an arbitrary set (not necessarily finite). An\n",
    "element of $ \\Omega $ will be called an __outcome__, and a subset of $ \\Omega $\n",
    "will be called an __event__.  A __probability function__ on $ \\Omega $ is a real\n",
    "function defined on the set of events of $ \\Omega $ that satisfies the following\n",
    "three properties (called the __Kolmogorov axioms__):\n",
    "\n",
    "1. (_Non-negativity_) For any event $ A $ in the sample space $ S $, the\n",
    "probability of $ A $ is non-negative; in symbols, $ P(A) \\ge 0 $.\n",
    "\n",
    "2. (_Unit Measure_) The probability of the entire sample space is 1, that is,\n",
    "$ P(\\Omega) = 1 $. This axiom implies that the probability that some outcome in\n",
    "the sample space will occur is certain.\n",
    "\n",
    "3. (_Countable Additivity_) For any countable or finite sequence of mutually\n",
    "exclusive events $ A_1, A_2, A_3, \\ldots $ (meaning no two events have any\n",
    "outcomes in common), the probability of the union of these events is equal to\n",
    "the sum of their individual probabilities. Symbolically, if $A_i \\cap A_j =\n",
    "\\emptyset$ for $i \\neq j$, then\n",
    "$$ P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i)\\,. $$\n",
    "\n",
    "Formally, a __sample space__ is a pair consisting of a set $ \\Omega $ and\n",
    "a probability function $ P $ as above. (Actually, our definition is not\n",
    "entirely correct because in the case where $ \\Omega $ is not\n",
    "discrete, it may not be possible to define $ P $ consistently over the\n",
    "set of _all_ subsets of $ \\Omega $. Because of this, we only require that $ P $\n",
    "be defined over a so-called $ \\sigma $-algebra, but we will ignore\n",
    "these technical difficulties here).\n",
    "\n",
    "Usually a sample space is described not in terms of the probability function\n",
    "$ P $ above (which is defined on a set of subsets of $ \\Omega $), but in terms\n",
    "of a simpler function $ f $ defined over $ \\Omega $ itself, called a\n",
    "__probability distribution__.\n",
    "\n",
    "A __discrete__ (or countable) sample space is one whose outcomes $ \\{x_1, x_2,\n",
    "\\cdots, x_n,\\cdots \\} $ are in one-to-one correspondence with set of natural\n",
    "numbers $ \\{1, 2, \\cdots, \\} $ or one of its subsets. In this case, $ P $ is\n",
    "completely determined by a suitable assignment of probabilities to each of the\n",
    "outcomes $ x_k $, by means of the distribution function $ f \\colon \\Omega \\to\n",
    "[0, 1] $. In this context, $ f $ is also called a _probability mass function_.\n",
    "Then for an arbitrary event $ A $, we have\n",
    "$$\n",
    "P(A) = \\sum_{x_k \\in A} f(x_k)\\qquad (A \\subset \\Omega).\n",
    "$$\n",
    "Note that this series is necessarily convergent because the total probability\n",
    "is $ 1 $.\n",
    "\n",
    "For a uncountable sample space $ \\Omega \\subset \\mathbb R^n $, we can again\n",
    "describe the probability function $ P $ by a simpler distribution function $ f\n",
    "\\colon \\Omega \\to [0, 1] $. However, this time we _integrate instead of summing_:\n",
    "$$\n",
    "P(A) = \\int_A f(x)\\,dx \\qquad (A \\subset \\Omega).\n",
    "$$\n",
    "In this context $ f $ is also called a __probability density function__.\n",
    "In most cases we assume that $ f $ is continuous, so that the integral above\n",
    "exists for any reasonable event $ A $ of $ \\Omega $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 9 $ Random number generation\n",
    "\n",
    "A common way to create an array is by populating it randomly. This is useful for\n",
    "example to initialize weights in a neural network; another common application is\n",
    "to randomly shuffle a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61408305 0.75204832 0.62026719 0.30220243 0.47913732] \n",
      "\n",
      "[[0.03751491 0.90405153 0.23338299]\n",
      " [0.99608328 0.78971066 0.50382658]]\n"
     ]
    }
   ],
   "source": [
    "# To generate random floating-point numbers between 0 and 1, use `rand`:\n",
    "random_vector = np.random.rand(5)  # 1D array of 5 random floats\n",
    "random_matrix = np.random.rand(2, 3)  # 2x3 array of random floats\n",
    "\n",
    "print(random_vector, '\\n')\n",
    "print(random_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `rand()` without arguments draws a random floating-point number in\n",
    "$ [0.0, 1.0) $ according to a uniform distribution, meaning that the probability\n",
    "that the result lies in an interval $ I \\subset [0, 1) $ is exactly the length $\n",
    "\\vert{I}\\vert $ of $ I $. More generally, the arguments of `rand` describe the\n",
    "size of the resulting array of random numbers, all drawn from this same distribution.\n",
    "In particular, note that every call to `rand` results in new numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy also provides functions to rearrange arrays randomly, which is especially useful in simulations and algorithms that require random sampling without replacement:\n",
    "\n",
    "* Shuffle (`shuffle`): Modifies an array in-place by shuffling its contents.\n",
    "* Permutation (`permutation`): Returns a new array that is a randomly permuted sequence of its input."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
